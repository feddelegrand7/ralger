% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/files_scrap.R
\name{xlsx_scrap}
\alias{xlsx_scrap}
\title{Scrape and download Excel xlsx files from a Web Page}
\usage{
xlsx_scrap(link, path = getwd(), askRobot = FALSE)
}
\arguments{
\item{link}{the link of the web page}

\item{path}{the path where to save the Excel xlsx files. Defaults to the current directory}

\item{askRobot}{logical. Should the function ask the robots.txt if we're allowed or not to scrape the web page ? Default is FALSE.}
}
\value{
called for the side effect of downloading Excel xlsx files from a website
}
\description{
Scrape and download Excel xlsx files from a Web Page
}
\examples{
\dontrun{

xlsx_scrap(
link = "https://www.rieter.com/investor-relations/results-and-presentations/financial-statements"
)

}
}
